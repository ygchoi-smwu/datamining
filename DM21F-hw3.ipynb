{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DM21F-hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGpbXsCvQgqxW7+Df8uoY7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ygchoi-smwu/datamining/blob/main/DM21F-hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHiljQlBS1q-"
      },
      "source": [
        "# HW 3 -- Due 10/15 (금) 6pm\n",
        "\n",
        "_데이터마이닝 (2021-2 숙명여대 통계학과)_,  Last update at 2021-10-01 06:30pm\n",
        "\n",
        "### 조원 이름 (e.g. 2112345 김눈송, 2112346 김송이, 2112347 김통계)\n",
        "\n",
        "### 본 숙제의 최종 검토 겸 제출자: ___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRECGVkxTpss"
      },
      "source": [
        "## 0. 일러두기\n",
        "\n",
        "- 풀이&제출법\n",
        "    - **이론 문항**\n",
        "        - 풀이를 수기작성하여 하나의 PDF 파일로 수합합니다.\n",
        "    - **코딩 문항** \n",
        "        - 풀이는 본 ipynb notebook에 수행합니다.\n",
        "        - **사고과정과 핵심 수치는 text chunk를 만들어 답하되, 제시된 수치를 재현할 수 있는 code chunk를 같이 첨부**하십시오.\n",
        "        - **실행 결과가 기록된 ipynb만 채점합니다.** 제출 전 반드시 ‘[수정]-[모든 출력 지우기]’ 및 ‘[런타임]-[다시 시작 및 모두 실행]’!\n",
        "    - 문항마다 풀이자와 기여자를 기록합니다. (free rider 예방이 목적입니다. 사람 수에 제한은 없고 자율적 판단하에 기록하십시오) \n",
        "    - PDF 한개, ipynb 한개를 최종검토&제출자가 업로드합니다.\n",
        "- 이번 숙제에서 허용되는 라이브러리는 아래와 같으나, 그 외에 스스로 필요하다고 판단하면 **ISLR 5장까지 등장한 라이브러리는 모두 사용 가능**합니다. 아래 code chunk를 실행시키세요.\n",
        "- 특별한 언급이 없으면 ISLR은 ISLR 2판을 지칭합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN_g9JN0WEy2"
      },
      "source": [
        "library(stringr)\n",
        "install.packages(c(\"ISLR2\", \"FNN\", \"pROC\"))\n",
        "library(ISLR2)\n",
        "library(FNN)\n",
        "library(pROC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMHvDTcXPqQ-"
      },
      "source": [
        "## (이론) 1. ISLR #4.6\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1ahW89lYzum"
      },
      "source": [
        "## (이론) 2. ISLR #4.9\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-gJLCczUq1S"
      },
      "source": [
        "## (이론) 3. ISLR #4.13  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRLOdkg3oabq"
      },
      "source": [
        "## (이론) 4. ISLR #5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLHRJ2gQ_o9i"
      },
      "source": [
        "## (이론) 5. ISLR #5.3 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhvl8vkd_pMU"
      },
      "source": [
        "## (코딩) 6. ISLR #5.5\n",
        "\n",
        "- (b)-iv **수정**: 추가적으로 정오행렬(confusion matrix), 민감도(sensitivity), 특이도(specificity), 정밀도(precision), 재현율(recall)을 계산하세요. **교육적 목적에서, 이 다섯 지표 계산은 하드코딩으로 하십시오.**\n",
        "- (b)-v **추가문제**: iv에서는 cutoff를 0.5로 정하여 오류지표를 계산하였다. cutoff를 정하지 않은 상태를 가정하고, ROC 곡선을 그린 후 AUROC를 계산하세요. pROC 패키지가 필요할 것입니다.\n",
        "- (c)를 풀 때는 위의 수정/추가사항을 모두 반영하여 답하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKIMTI1sYtWU"
      },
      "source": [
        "## (코딩) 7. ISLR #5.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILxHmUQ5Sxvb"
      },
      "source": [
        "## (코딩) 8. ISLR #5.8 \n",
        "\n",
        "- **수정**: 문제에서는 LOOCV를 수행하라고 써있으나, 모두 5-fold CV로 변경하고 푸세요.\n",
        "- (f) **수정**: ``Do these results agree with the conclusions drawn based on the cross-validation results?''에 대하여는 답할 필요 없습니다. (이미 수행하셨으므로)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kQyU-2fGMaW"
      },
      "source": [
        "## (코딩) 9.\n",
        "\n",
        "이 문제에서는 하드코딩으로 (`R`의 기본 연산기능만 이용하여)으로 **뉴턴 방법 (Newton's method)** (4단원 강의노트 20쪽 참조)을 구현하여 로지스틱 회귀분석의 회귀계수($\\beta$)를 추정합니다. \n",
        "\n",
        "\n",
        "도움을 드리기 위하여 코드 몇 줄을 앞뒤로 삽입하였습니다. 주로 (1) 알고리즘이 알맞게 계산되었는지 체크하는 코드들,  (2) 반복 갱신 알고리즘에 흔히 사용되는 코드들입니다.\n",
        "**아래에서 \"## 여기에 코드를 채우세요 ##\" 부분을 채워 알고리즘을 완성하세요.** 맨 아래 두 줄로 여러분들의 코드를 테스트하실 수 있습니다.\n",
        "\n",
        "- Note 1: 이 문제는 기계학습을 위한 수치적 최적화에 익숙해지기 위한 첫 걸음입니다. $\\beta$의 추정 과정은 $\\beta$에 대한 다변수함수의 수치적 최소화(numerical minimization) 과정으로 볼수 있음을 3,4단원에서 강조하였습니다. 모수적 방법론에서  $f(x)$를 $x$의 복잡한 함수로 모형화할수록(**특히 딥러닝**) 훈련(최적화) 과정의 모니터링이 중요해집니다. 보통 딥러닝에서는 계산속도를 위하여 뉴턴 방법 대신 확률적 경사 하강법(stochastic gradient descent) 알고리즘을 사용합니다.\n",
        "\n",
        "- Note 2: `glm()`도 뉴턴 방법을 사용합니다. `glm` 적합 후 `summary`를 찍어보시면 가장 마지막 줄에 `Number of Fisher Scoring Iterations`이 출력됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvRutGQdGJA2"
      },
      "source": [
        "############################################################\n",
        "############## 테스트용 시뮬레이션 데이터 생성 #############\n",
        "############################################################\n",
        "# define the inverse logit function\n",
        "expit = function(t) return(exp(t) / (1 + exp(t)))\n",
        "# sample size is n=10000, the number of variables is p=2\n",
        "n = 10000\n",
        "set.seed(1)\n",
        "X1 = rnorm(n)\n",
        "set.seed(2)\n",
        "X2 = rnorm(n)\n",
        "# the true coefficients. beta0 = 1, beta1 = -2, beta2 = 1\n",
        "theta.true = c(1, -2, 1)\n",
        "# beta0 + beta1 * X1 + beta2 * X2\n",
        "x_theta = theta.true[1] + theta.true[2] * X1 + theta.true[3] * X2           \n",
        "# Y|X1,X2 follows the Bernoulli distribution \n",
        "#    with the success probability as expit(beta0 + beta1 * X1 + beta2 * X2)\n",
        "set.seed(3)\n",
        "y = rbinom(n=n, size=1, prob=expit(x_theta)) \n",
        "\n",
        "\n",
        "# 보조적인 개체\n",
        "# 상수항(1)을 포함한 자료행렬. 아래 반복문에 필요할 겁니다.\n",
        "Xmat = cbind(1, X1, X2)\n",
        "\n",
        "############################################################\n",
        "# 반복적 국소 이차 근사로 로지스틱 회귀분석 계수 구하기 시작\n",
        "############################################################\n",
        "# 최대 반복수. 코딩오류로 인한 무한루프를 막기 위하여 실무적으로 설정해줌\n",
        "MAXITER = 1000\n",
        "# 기존값과 갱신값이 tol 미만이면 수렴을 선언할 계획임. 실무적으로 10^-4 ~ 10^-6을 애용합니다.\n",
        "tol = 10^-8\n",
        "# 반복 갱신될 theta의 초기값 설정 \n",
        "theta.old = c(0, 0, 0) \n",
        "# 반복 알고리즘 시작\n",
        "for (t in 1:MAXITER) {\n",
        "  # 모니터링을 위한 프린트문\n",
        "  cat(sprintf(\"============ Iteration %d ==========\", t))\n",
        "  \n",
        "  ###########################################################\n",
        "  # 여기에 코드를 채우세요.\n",
        "  # \t- R의 행렬/벡터 계산 기능을 이용하여, theta.old와 Xmat과 y로부터 theta.new를 계산하십시오.\n",
        "  #   - 강의노트 4장의 20쪽 참조\n",
        "  #   - 계산과정의 중간값들을 추가 변수로 정의해도 괜찮습니다.\n",
        "  ###########################################################\n",
        "  \n",
        "  # diff는 theta.new와 theta.old간 유클리드 거리로 정의하였음\n",
        "  diff = sqrt(sum((theta.new - theta.old)^2))\n",
        "  sprintf(\"L2 difference between theta.new and theta.old: %.8f\\n\", diff)\t\t\n",
        "  \n",
        "  # theta가 충분히 수렴한 듯하면 \n",
        "  # 가장 최신의 theta.new를 theta의 추정값(theta.hat)으로 제시 후\n",
        "  # 반복문 빠져나가기\n",
        "  if (diff < tol) {\t\n",
        "   cat(sprintf(\"Fisher scoring algorithm converged with %d iterations\\n\", t))\n",
        "   theta.hat = theta.new\n",
        "   break\n",
        "  }\n",
        "  \n",
        "  # 수렴 실패하였을 경우 메시지 출력\n",
        "  if (t == MAXITER) cat(\"Did not converge\\n\")\n",
        "}\n",
        "\n",
        "# n을 크게 설정할수록 theta.true에 가까운 값이 출력되어야 합니다. (theta.hat이 MLE이므로 consistency에 의하여)\n",
        "print(theta.hat)\n",
        "\n",
        "# 위의 theta.hat과 동일한 값이 출력되어야 합니다.\n",
        "summary(glm(y~ X1 + X2, family=binomial))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}